{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf you are running locally then \\n1. reboot your local machine\\n2. create an environment called 'colab' using anaconda prompt\\nif you have a gpu\\nconda create -n colab python tensorflow-gpu \\nif not \\nconda create -n colab python tensorflow\\n3. to install jupyter notebook\\nconda install jupyter notebook\\n4. to go to the 'colab' environment\\nactivate colab\\n5. change file path to locate this notebook and then type 'jupyter notebook'\\n\\nIf you use colab\\n1. save the data file in your google drive\\n2. goto colab and start running the code\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you are running locally then \n",
    "1. reboot your local machine\n",
    "2. create an environment called 'colab' using anaconda prompt\n",
    "if you have a gpu\n",
    "conda create -n colab python tensorflow-gpu \n",
    "if not \n",
    "conda create -n colab python tensorflow\n",
    "3. to install jupyter notebook\n",
    "conda install jupyter notebook\n",
    "4. to go to the 'colab' environment\n",
    "activate colab\n",
    "5. change file path to locate this notebook and then type 'jupyter notebook'\n",
    "\n",
    "If you use colab\n",
    "1. save the data file in your google drive\n",
    "2. goto colab and start running the code\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install prominent libraries with specific versions\n",
    "\n",
    "#!pip install tensorflow==1.15.0\n",
    "#!pip install keras==2.2.4-tf\n",
    "#!pip install pandas==0.25.1\n",
    "#!pip install sklearn==0.21.3\n",
    "#!pip install matplotlib==3.2.1\n",
    "#!pip install hyperas\n",
    "#!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "sJnnN6xG_yaM",
    "outputId": "b6420ce8-7f05-440c-e2ce-479b5bc1cafc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function                                                                                                                                                                                                                                                                                                                              # from tensorflow.contrib.rnn import *import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU, Input, Activation, Flatten, BatchNormalization, Reshape,Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, History\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import*\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta\n",
    "from keras.utils import np_utils, to_categorical\n",
    "import pandas as pd\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from collections import Counter\n",
    "import operator\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import hyperas\n",
    "import hyperopt\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperas import optim\n",
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from keras.layers import Conv1D, MaxPooling1D, ZeroPadding1D\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import pandas_market_calendars as mcal \n",
    "import datetime \n",
    "from datetime import timedelta  \n",
    "import yfinance as yf\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.__version__ =  1.10.0\n",
      "keras.__version__ =  2.1.6-tf\n",
      "sklearn.__version__ =  0.21.3\n",
      "numpy.__version__ =  1.17.0\n",
      "pandas.__version__ =  1.0.1\n",
      "matplotlib.__version__ =  3.1.3\n",
      "Last run date :  07-28-2020\n"
     ]
    }
   ],
   "source": [
    "#Get library versions\n",
    "print(\"tensorflow.__version__ = \", tf.__version__)\n",
    "print(\"keras.__version__ = \", keras.__version__)\n",
    "import sklearn \n",
    "print(\"sklearn.__version__ = \", sklearn.__version__)\n",
    "print(\"numpy.__version__ = \", np.__version__)\n",
    "print(\"pandas.__version__ = \", pd.__version__)\n",
    "import matplotlib\n",
    "print(\"matplotlib.__version__ = \", matplotlib.__version__)\n",
    "now = datetime.datetime.now()\n",
    "print (\"Last run date : \", now.strftime(\"%m-%d-%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed to generate reproduceable results\n",
    "from numpy.random import seed\n",
    "seed(56)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(56)\n",
    "random.seed(56)\n",
    "os.environ['PYTHONHASHSEED']=str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they do not exist\n",
    "def build_path(dirName):\n",
    "    try:\n",
    "        os.makedirs(dirName)    \n",
    "        print(\"Directory \" , dirName ,  \" Created \")\n",
    "    except:\n",
    "        print(\"Directory \" , dirName ,  \" already exists\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9555189157885764567\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7139449242\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8503368834181335048\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# to check if GPU is getting used locally.....you need to see CPU as well as GPU in the output\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOs6JYN__51O"
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "  i = 7 # the label target - number of days to predict from the input date\n",
    "  p = 7 #Number of days for target calculated in the data set\n",
    "  batch_size=512\n",
    "  CLASSES = 2\n",
    "  time_steps = 28\n",
    "  ticker='^GSPC'\n",
    "    \n",
    "  # read data\n",
    "  df=pd.read_csv('../^GSPC_7_days_0_return_dtw.csv', index_col = 0, parse_dates = True)\n",
    "    \n",
    "  #add additional rolling mean data  \n",
    "  rm_window =30\n",
    "  rolling_mean = []\n",
    "  for a in range(2,rm_window+1):\n",
    "    df[ticker+'rm_'+str(a)] = df[ticker].rolling(window=rm_window,center=False).mean()\n",
    "    rolling_mean.append(ticker+'rm_'+str(a))\n",
    "    \n",
    "  # create label\n",
    "  targets=pd.DataFrame([])\n",
    "  for j in range (1, p+1):\n",
    "    targets=targets.append(df[ticker+'_{}d_target'.format(j)])\n",
    "    targets=targets.append(df[ticker+'_{}d'.format(j)])\n",
    "  targets=targets.T\n",
    "  df=df.drop(targets.columns, axis=1)\n",
    "  df=df[rm_window:-i]\n",
    "  targets=targets[rm_window:-i]\n",
    "  y=targets['^GSPC_{}d_target'.format(i)]\n",
    "\n",
    "  #check for NaN and remove\n",
    "  df.isna().mean().sum()\n",
    "  y.isna().mean().sum()\n",
    "  remove_list=[]\n",
    "  for i in df.isnull().any().iteritems():\n",
    "    if i[1] == True:\n",
    "      remove_list.append(i[0])\n",
    "  df=df.drop(remove_list, axis=1)\n",
    "  df.isnull().any().mean()\n",
    " \n",
    "  # add percent change\n",
    "  df=df.pct_change()\n",
    "  df=df.replace([np.inf, -np.inf],np.nan) \n",
    "  df.fillna(0, inplace=True)\n",
    "  df.isnull().any().mean()\n",
    "    \n",
    "  # apply preprocessing \n",
    "  x_scaler=RobustScaler()\n",
    "  x = x_scaler.fit_transform(df)\n",
    "  # x_pred = x_scaler.fit_transform(x_pred)\n",
    "  del df\n",
    "  y=y.values\n",
    "    \n",
    "  # apply time steps\n",
    "  def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "      v = X[i:(i + time_steps)]\n",
    "      Xs.append(v)\n",
    "      ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "  x, y = create_dataset(x, y, time_steps)\n",
    "\n",
    "  # create train and test dataset\n",
    "  x_train, x_test, y_train, y_test=train_test_split(x,y, train_size=0.7, random_state=54)\n",
    "  x_train = x_train.astype('float32')\n",
    "  x_test = x_test.astype('float32')\n",
    "  #y_train = y_train.astype('float32')\n",
    "  #y_test = y_test.astype('float32')\n",
    "  y_train = np_utils.to_categorical(y_train, CLASSES,dtype='float32')\n",
    "  y_test = np_utils.to_categorical(y_test, CLASSES, dtype='float32')\n",
    "    \n",
    "  # adjustment for batch_size\n",
    "  train_start = x_train.shape[0]%batch_size\n",
    "  test_start = x_test.shape[0]%batch_size\n",
    "  x_train = x_train[train_start:]\n",
    "  y_train = y_train[train_start:]\n",
    "  x_test = x_test[test_start:]\n",
    "  y_test = y_test[test_start:]\n",
    "\n",
    "  return x_train, x_test, y_train, y_test, batch_size, time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc8hDOIbADgQ"
   },
   "outputs": [],
   "source": [
    "def create_self_attention_lstm_model(x_train, x_test, y_train, y_test, batch_size, time_steps):\n",
    "    \n",
    "    class Attention(Layer):\n",
    "        def __init__(self, step_dim,\n",
    "                     W_regularizer=None, b_regularizer=None,\n",
    "                     W_constraint=None, b_constraint=None,\n",
    "                     bias=True, **kwargs):\n",
    "            self.supports_masking = True\n",
    "            self.init = initializers.get('glorot_uniform')\n",
    "            self.W_regularizer = regularizers.get(W_regularizer)\n",
    "            self.b_regularizer = regularizers.get(b_regularizer)\n",
    "            self.W_constraint = constraints.get(W_constraint)\n",
    "            self.b_constraint = constraints.get(b_constraint)\n",
    "            self.bias = bias\n",
    "            self.step_dim = step_dim\n",
    "            self.features_dim = 0\n",
    "            super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            assert len(input_shape) == 3\n",
    "            self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer=self.init,\n",
    "                                     name='{}_W'.format(self.name),\n",
    "                                     regularizer=self.W_regularizer,\n",
    "                                     constraint=self.W_constraint)\n",
    "            self.features_dim = input_shape[-1]\n",
    "            if self.bias:\n",
    "                self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                         initializer='zero',\n",
    "                                         name='{}_b'.format(self.name),\n",
    "                                         regularizer=self.b_regularizer,\n",
    "                                         constraint=self.b_constraint)\n",
    "            else:\n",
    "                self.b = None\n",
    "            self.built = True\n",
    "\n",
    "        def compute_mask(self, input, input_mask=None):\n",
    "            return None\n",
    "\n",
    "        def call(self, x, mask=None):\n",
    "            features_dim = self.features_dim\n",
    "            step_dim = self.step_dim\n",
    "            eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),\n",
    "                            (-1, step_dim))\n",
    "            if self.bias:\n",
    "                eij += self.b\n",
    "            eij = K.tanh(eij)\n",
    "            a = K.exp(eij)\n",
    "            if mask is not None:\n",
    "                a *= K.cast(mask, K.floatx())\n",
    "            a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "            a = K.expand_dims(a)\n",
    "            weighted_input = x * a\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return input_shape[0],  self.features_dim\n",
    "   \n",
    "    \n",
    "    x0 = Input(shape=(x_train.shape[1], x_train.shape[2]), name='x0')\n",
    "    x1 = (LSTM(units = {{choice([64,128])}}, \n",
    "               kernel_regularizer=regularizers.l2({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}), \n",
    "               bias_regularizer=regularizers.l1({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}),\n",
    "              return_sequences=True))(x0)\n",
    "    x2 = LeakyReLU(alpha=({{choice([0.01, 0.05,0.1])}}))(x1)\n",
    "    x3 = BatchNormalization()(x2)\n",
    "    x4 = Dropout({{uniform(0, 1)}})(x3)\n",
    "    # x3 = (LSTM(24, kernel_regularizer=regularizers.l2(0.03),bias_regularizer=regularizers.l1(0.03),\n",
    "    #           return_sequences=True))(x2)\n",
    "    # x3 = LeakyReLU(alpha=0.05)(x3)\n",
    "    # x3 = BatchNormalization()(x3)\n",
    "    # x4 = Dropout(0.6)(x3)\n",
    "    x5 = Attention(time_steps)(x4)\n",
    "    x6 = Dense(units = {{choice([32,64,128])}}, \n",
    "               kernel_regularizer=regularizers.l2({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}), \n",
    "               bias_regularizer=regularizers.l1({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}),\n",
    "               activation='relu')(x5)\n",
    "    x7 = Dropout({{uniform(0, 1)}})(x6)\n",
    "    x8 = Dense(units = {{choice([16, 32, 64])}}, \n",
    "               kernel_regularizer=regularizers.l2({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}), \n",
    "               bias_regularizer=regularizers.l1({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}),\n",
    "               activation='relu')(x7)\n",
    "    \n",
    "    x9 = Dropout({{uniform(0, 1)}})(x8)\n",
    "    x10 = BatchNormalization()(x9)\n",
    "    x11 = Dense(units = {{choice([8, 16, 32])}},\n",
    "                kernel_regularizer=regularizers.l2({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}), \n",
    "               bias_regularizer=regularizers.l1({{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}),\n",
    "                activation='relu')(x10)\n",
    "    x12 = BatchNormalization()(x11)\n",
    "    x13 = Dropout(0.3)(x12)\n",
    "    outp = Dense(2, activation='softmax')(x13)\n",
    "\n",
    "    model = Model(inputs=[x0], outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer = Adam(lr={{choice([1.0, 0.1,0.01,0.001])}}))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    result = model.fit(x_train,y_train,batch_size=batch_size,epochs=100,validation_data=(x_test, y_test), verbose=0)\n",
    "    \n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "\n",
    "    print('Best validation acc of epoch:', result.history['val_acc'])\n",
    "    print('Train acc of epoch:', result.history['acc'])\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HcxtusN4AK8y",
    "outputId": "43d151c2-cf3b-4ab8-e95f-7c48cef1da49",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8704, 28, 2782) (8704, 2) (3584, 28, 2782) (3584, 2)\n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "x0 (InputLayer)              (None, 28, 2782)          0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "lstm_1 (LSTM)                (None, 28, 64)            728832                                                          \n",
      "_________________________________________________________________                                                      \n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 64)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_1 (Batch (None, 28, 64)            256                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_1 (Dropout)          (None, 28, 64)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "attention_1 (Attention)      (None, 64)                92                                                              \n",
      "_________________________________________________________________                                                      \n",
      "dense_1 (Dense)              (None, 32)                2080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_2 (Dropout)          (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_2 (Dense)              (None, 16)                528                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_3 (Dropout)          (None, 16)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_2 (Batch (None, 16)                64                                                              \n",
      "_________________________________________________________________                                                      \n",
      "dense_3 (Dense)              (None, 8)                 136                                                             \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_3 (Batch (None, 8)                 32                                                              \n",
      "_________________________________________________________________                                                      \n",
      "dropout_4 (Dropout)          (None, 8)                 0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_4 (Dense)              (None, 2)                 18                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 732,038                                                                                                  \n",
      "Trainable params: 731,862                                                                                              \n",
      "Non-trainable params: 176                                                                                              \n",
      "_________________________________________________________________                                                      \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5326450892857143, 0.5496651785714286, 0.5446428571428571, 0.5560825892857143, 0.5703125, 0.5758928571428571, 0.5680803571428571, 0.5661272321428571, 0.5641741071428571, 0.56640625, 0.5703125, 0.5694754464285714, 0.568359375, 0.5655691964285714, 0.5675223214285714, 0.5686383928571429, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5678013392857143, 0.5680803571428571, 0.5697544642857143, 0.5694754464285714, 0.5680803571428571, 0.5680803571428571, 0.5806361607142857, 0.5680803571428571, 0.5680803571428571, 0.5887276785714286, 0.5809151785714286, 0.5851004464285714, 0.5756138392857143, 0.5890066964285714, 0.5890066964285714, 0.5979352678571429, 0.6012834821428571, 0.6079799107142857, 0.5998883928571429, 0.6046316964285714, 0.6018415178571429, 0.6021205357142857, 0.5923549107142857, 0.5881696428571429, 0.6071428571428571, 0.6177455357142857, 0.583984375, 0.5831473214285714, 0.5929129464285714, 0.5959821428571429, 0.6057477678571429, 0.6057477678571429, 0.603515625, 0.5806361607142857, 0.6088169642857143, 0.6236049107142857, 0.6015625, 0.6143973214285714, 0.5856584821428571, 0.6266741071428571, 0.6118861607142857, 0.6238839285714286, 0.6090959821428571, 0.6116071428571429, 0.5951450892857143, 0.615234375, 0.6135602678571429, 0.6138392857142857, 0.5923549107142857, 0.6196986607142857, 0.6099330357142857, 0.6177455357142857]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5050551470588235, 0.5094209558823529, 0.505859375, 0.5071231617647058, 0.5229779411764706, 0.5173483455882353, 0.5252757352941176, 0.5183823529411765, 0.5159696691176471, 0.5242417279411765, 0.5226332720588235, 0.5287224264705882, 0.5303308823529411, 0.5268841911764706, 0.5275735294117647, 0.5321691176470589, 0.5365349264705882, 0.5342371323529411, 0.5349264705882353, 0.5299862132352942, 0.5367647058823529, 0.5473345588235294, 0.5471047794117647, 0.5394071691176471, 0.5503216911764706, 0.5467601102941176, 0.5503216911764706, 0.5471047794117647, 0.5606617647058824, 0.55859375, 0.5526194852941176, 0.5627297794117647, 0.5635340073529411, 0.5619255514705882, 0.5666360294117647, 0.5688189338235294, 0.5654871323529411, 0.5701976102941176, 0.572265625, 0.5767463235294118, 0.5811121323529411, 0.5741038602941176, 0.5766314338235294, 0.5786994485294118, 0.5770909926470589, 0.5798483455882353, 0.5897288602941176, 0.5858226102941176, 0.5894990808823529, 0.5886948529411765, 0.5983455882352942, 0.5928308823529411, 0.5915670955882353, 0.5896139705882353, 0.5924862132352942, 0.5923713235294118, 0.5981158088235294, 0.6071920955882353, 0.6013327205882353, 0.6027113970588235, 0.5953584558823529, 0.6051240808823529, 0.5968520220588235, 0.5993795955882353, 0.6094898897058824, 0.6094898897058824, 0.6025965073529411, 0.61328125, 0.6068474264705882, 0.5977711397058824, 0.5988051470588235, 0.6046645220588235, 0.5938648897058824, 0.6037454044117647, 0.6063878676470589, 0.6032858455882353, 0.6047794117647058, 0.5980009191176471, 0.5945542279411765, 0.6081112132352942, 0.6038602941176471, 0.6053538602941176, 0.6013327205882353, 0.6092601102941176, 0.60546875, 0.6100643382352942, 0.6160386029411765, 0.6124770220588235, 0.6096047794117647, 0.59765625, 0.6123621323529411, 0.6066176470588235, 0.6088005514705882, 0.6063878676470589, 0.6104090073529411, 0.6078814338235294, 0.6001838235294118, 0.6138556985294118, 0.6108685661764706, 0.6032858455882353]\n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "x0 (InputLayer)              (None, 28, 2782)          0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "lstm_2 (LSTM)                (None, 28, 128)           1490432                                                         \n",
      "_________________________________________________________________                                                      \n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_4 (Batch (None, 28, 128)           512                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_5 (Dropout)          (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "attention_2 (Attention)      (None, 128)               156                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_5 (Dense)              (None, 64)                8256                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_6 (Dropout)          (None, 64)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_6 (Dense)              (None, 32)                2080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_7 (Dropout)          (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_5 (Batch (None, 32)                128                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_7 (Dense)              (None, 8)                 264                                                             \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_6 (Batch (None, 8)                 32                                                              \n",
      "_________________________________________________________________                                                      \n",
      "dropout_8 (Dropout)          (None, 8)                 0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_8 (Dense)              (None, 2)                 18                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 1,501,878                                                                                                \n",
      "Trainable params: 1,501,542                                                                                            \n",
      "Non-trainable params: 336                                                                                              \n",
      "_________________________________________________________________                                                      \n",
      "Best validation acc of epoch:                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5541294642857143, 0.53515625, 0.5142299107142857, 0.5267857142857143, 0.5223214285714286, 0.5287388392857143, 0.5329241071428571, 0.5658482142857143, 0.5666852678571429, 0.5675223214285714, 0.5680803571428571, 0.5680803571428571, 0.5672433035714286, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.62109375, 0.6188616071428571, 0.6049107142857143, 0.6255580357142857, 0.6143973214285714, 0.6213727678571429, 0.6085379464285714, 0.6227678571428571, 0.6169084821428571, 0.6224888392857143, 0.623046875, 0.625, 0.6141183035714286, 0.6157924107142857, 0.6149553571428571, 0.6291852678571429, 0.6213727678571429, 0.6244419642857143, 0.630859375, 0.6194196428571429, 0.6258370535714286, 0.6244419642857143, 0.6428571428571429, 0.6344866071428571, 0.6297433035714286, 0.6238839285714286, 0.6297433035714286, 0.6339285714285714, 0.6305803571428571, 0.6356026785714286, 0.6316964285714286, 0.6330915178571429, 0.6283482142857143, 0.640625, 0.6311383928571429, 0.6342075892857143, 0.6397879464285714, 0.6489955357142857, 0.6448102678571429, 0.650390625, 0.6361607142857143, 0.626953125, 0.6344866071428571, 0.6322544642857143, 0.6395089285714286, 0.6389508928571429, 0.6409040178571429, 0.6534598214285714, 0.6431361607142857, 0.6487165178571429, 0.6439732142857143, 0.6409040178571429]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4990808823529412, 0.49919577205882354, 0.5099954044117647, 0.5221737132352942, 0.513671875, 0.5251608455882353, 0.5089613970588235, 0.5234375, 0.5360753676470589, 0.537109375, 0.5320542279411765, 0.5346966911764706, 0.5387178308823529, 0.5435431985294118, 0.5467601102941176, 0.5458409926470589, 0.5484834558823529, 0.5526194852941176, 0.5464154411764706, 0.5568704044117647, 0.5554917279411765, 0.5495174632352942, 0.5557215073529411, 0.5604319852941176, 0.5596277573529411, 0.5595128676470589, 0.5634191176470589, 0.5641084558823529, 0.5554917279411765, 0.5583639705882353, 0.5628446691176471, 0.5602022058823529, 0.5645680147058824, 0.5706571691176471, 0.5687040441176471, 0.5799632352941176, 0.5804227941176471, 0.5749080882352942, 0.5799632352941176, 0.5877757352941176, 0.5873161764705882, 0.5905330882352942, 0.5786994485294118, 0.5789292279411765, 0.5928308823529411, 0.6007582720588235, 0.5961626838235294, 0.5990349264705882, 0.6061580882352942, 0.60546875, 0.6131663602941176, 0.5992647058823529, 0.6117876838235294, 0.6053538602941176, 0.6169577205882353, 0.6176470588235294, 0.6145450367647058, 0.6115579044117647, 0.6124770220588235, 0.6198299632352942, 0.6231617647058824, 0.6216681985294118, 0.6148897058823529, 0.6129365808823529, 0.6140854779411765, 0.6268382352941176, 0.6310891544117647, 0.6401654411764706, 0.6251148897058824, 0.6382123161764706, 0.6309742647058824, 0.6299402573529411, 0.6270680147058824, 0.6251148897058824, 0.6328125, 0.6326976102941176, 0.6392463235294118, 0.6385569852941176, 0.6363740808823529, 0.6360294117647058, 0.6394761029411765, 0.6337316176470589, 0.6399356617647058, 0.6320082720588235, 0.6384420955882353, 0.6393612132352942, 0.6501608455882353, 0.6444163602941176, 0.6422334558823529, 0.634765625, 0.6437270220588235, 0.6436121323529411, 0.6348805147058824, 0.6472886029411765, 0.6525735294117647, 0.6423483455882353, 0.6492417279411765, 0.6478630514705882, 0.634765625, 0.6548713235294118]\n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "x0 (InputLayer)              (None, 28, 2782)          0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "lstm_3 (LSTM)                (None, 28, 64)            728832                                                          \n",
      "_________________________________________________________________                                                      \n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 28, 64)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_7 (Batch (None, 28, 64)            256                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_9 (Dropout)          (None, 28, 64)            0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "attention_3 (Attention)      (None, 64)                92                                                              \n",
      "_________________________________________________________________                                                      \n",
      "dense_9 (Dense)              (None, 64)                4160                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_10 (Dropout)         (None, 64)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_10 (Dense)             (None, 32)                2080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_11 (Dropout)         (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_8 (Batch (None, 32)                128                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_11 (Dense)             (None, 32)                1056                                                            \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_9 (Batch (None, 32)                128                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_12 (Dropout)         (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_12 (Dense)             (None, 2)                 66                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 736,798                                                                                                  \n",
      "Trainable params: 736,542                                                                                              \n",
      "Non-trainable params: 256                                                                                              \n",
      "_________________________________________________________________                                                      \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.48604910714285715, 0.5008370535714286, 0.5340401785714286, 0.5518973214285714, 0.556640625, 0.56640625, 0.5703125, 0.5672433035714286, 0.5678013392857143, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.56640625, 0.5675223214285714, 0.48995535714285715, 0.455078125, 0.455078125, 0.4790736607142857, 0.5200892857142857, 0.5546875, 0.5873325892857143, 0.62890625, 0.6202566964285714, 0.6356026785714286, 0.6291852678571429, 0.6400669642857143, 0.6336495535714286, 0.625, 0.6286272321428571, 0.6392299107142857, 0.6442522321428571, 0.6180245535714286, 0.6375558035714286, 0.6428571428571429, 0.6361607142857143, 0.6244419642857143, 0.625, 0.6325334821428571, 0.63671875, 0.6356026785714286, 0.6436941964285714, 0.6261160714285714, 0.6185825892857143, 0.642578125, 0.6372767857142857, 0.6356026785714286, 0.6414620535714286, 0.6277901785714286, 0.6467633928571429, 0.6316964285714286, 0.6431361607142857, 0.6517857142857143, 0.6344866071428571, 0.6316964285714286, 0.6336495535714286, 0.6434151785714286, 0.6023995535714286, 0.6325334821428571, 0.6364397321428571, 0.646484375, 0.6473214285714286, 0.6258370535714286, 0.6559709821428571, 0.6529017857142857, 0.6244419642857143, 0.6431361607142857, 0.6383928571428571, 0.62890625, 0.6180245535714286, 0.6280691964285714, 0.6515066964285714, 0.65234375, 0.638671875, 0.6180245535714286, 0.6238839285714286, 0.6383928571428571, 0.6389508928571429, 0.6467633928571429, 0.6381138392857143, 0.6263950892857143, 0.6411830357142857, 0.6397879464285714, 0.6369977678571429, 0.6342075892857143, 0.6436941964285714, 0.6311383928571429, 0.6369977678571429, 0.6436941964285714, 0.6515066964285714, 0.6487165178571429, 0.623046875, 0.6495535714285714, 0.6358816964285714, 0.6568080357142857, 0.6579241071428571]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4893152573529412, 0.5050551470588235, 0.5109145220588235, 0.5161994485294118, 0.5135569852941176, 0.5291819852941176, 0.5306755514705882, 0.5304457720588235, 0.5307904411764706, 0.5349264705882353, 0.5287224264705882, 0.5404411764705882, 0.5390625, 0.5466452205882353, 0.5364200367647058, 0.5430836397058824, 0.5425091911764706, 0.5522748161764706, 0.5558363970588235, 0.564453125, 0.5710018382352942, 0.5695082720588235, 0.5747931985294118, 0.5913373161764706, 0.5970818014705882, 0.6075367647058824, 0.6218979779411765, 0.6346507352941176, 0.6267233455882353, 0.6443014705882353, 0.6352251838235294, 0.6545266544117647, 0.6363740808823529, 0.6390165441176471, 0.6468290441176471, 0.6488970588235294, 0.6369485294117647, 0.6408547794117647, 0.6362591911764706, 0.6467141544117647, 0.6515395220588235, 0.6536075367647058, 0.6547564338235294, 0.6492417279411765, 0.6560202205882353, 0.6518841911764706, 0.6526884191176471, 0.6462545955882353, 0.6353400735294118, 0.6488970588235294, 0.646484375, 0.6585477941176471, 0.6530330882352942, 0.6528033088235294, 0.6590073529411765, 0.6548713235294118, 0.6555606617647058, 0.6571691176470589, 0.6639476102941176, 0.6537224264705882, 0.6420036764705882, 0.6424632352941176, 0.6431525735294118, 0.6499310661764706, 0.6475183823529411, 0.6462545955882353, 0.6491268382352942, 0.6560202205882353, 0.6521139705882353, 0.658203125, 0.6553308823529411, 0.6579733455882353, 0.6618795955882353, 0.6613051470588235, 0.6513097426470589, 0.6534926470588235, 0.6536075367647058, 0.6591222426470589, 0.6518841911764706, 0.6552159926470589, 0.6544117647058824, 0.6516544117647058, 0.6532628676470589, 0.6479779411764706, 0.6519990808823529, 0.6584329044117647, 0.6552159926470589, 0.6359145220588235, 0.6557904411764706, 0.6610753676470589, 0.6545266544117647, 0.6563648897058824, 0.6673943014705882, 0.6633731617647058, 0.6571691176470589, 0.6564797794117647, 0.6497012867647058, 0.6545266544117647, 0.6557904411764706, 0.6545266544117647]\n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "x0 (InputLayer)              (None, 28, 2782)          0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "lstm_4 (LSTM)                (None, 28, 128)           1490432                                                         \n",
      "_________________________________________________________________                                                      \n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_10 (Batc (None, 28, 128)           512                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_13 (Dropout)         (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "attention_4 (Attention)      (None, 128)               156                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_13 (Dense)             (None, 64)                8256                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_14 (Dropout)         (None, 64)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_14 (Dense)             (None, 32)                2080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_15 (Dropout)         (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_11 (Batc (None, 32)                128                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_15 (Dense)             (None, 32)                1056                                                            \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_12 (Batc (None, 32)                128                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_16 (Dropout)         (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_16 (Dense)             (None, 2)                 66                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 1,502,814                                                                                                \n",
      "Trainable params: 1,502,430                                                                                            \n",
      "Non-trainable params: 384                                                                                              \n",
      "_________________________________________________________________                                                      \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5678013392857143, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.576171875, 0.5714285714285714, 0.568359375, 0.5859375, 0.5731026785714286, 0.5680803571428571, 0.5767299107142857, 0.5797991071428571, 0.5711495535714286, 0.568359375, 0.5691964285714286, 0.5736607142857143, 0.5756138392857143, 0.5703125, 0.5700334821428571, 0.5719866071428571, 0.5675223214285714, 0.5680803571428571, 0.5962611607142857, 0.6063058035714286, 0.5856584821428571, 0.5851004464285714, 0.607421875, 0.5867745535714286, 0.6096540178571429, 0.578125, 0.5993303571428571, 0.5672433035714286, 0.5912388392857143, 0.568359375, 0.5739397321428571, 0.5940290178571429, 0.5864955357142857, 0.5705915178571429, 0.5680803571428571, 0.5680803571428571, 0.5678013392857143, 0.6026785714285714, 0.5756138392857143, 0.5842633928571429, 0.5962611607142857, 0.5694754464285714, 0.568359375, 0.5864955357142857, 0.5973772321428571, 0.5856584821428571, 0.5809151785714286, 0.5672433035714286, 0.5700334821428571, 0.5817522321428571, 0.58984375, 0.5750558035714286, 0.6082589285714286, 0.5926339285714286, 0.6007254464285714, 0.5979352678571429, 0.5770089285714286, 0.5700334821428571, 0.5680803571428571, 0.5753348214285714, 0.5680803571428571, 0.583984375, 0.6032366071428571, 0.5998883928571429, 0.5876116071428571, 0.5711495535714286, 0.5680803571428571, 0.5993303571428571, 0.6090959821428571, 0.5758928571428571, 0.5965401785714286, 0.5680803571428571, 0.5719866071428571, 0.5731026785714286, 0.5697544642857143, 0.5680803571428571, 0.6001674107142857, 0.5714285714285714, 0.5954241071428571, 0.5680803571428571, 0.5862165178571429, 0.5837053571428571, 0.5973772321428571, 0.5694754464285714, 0.5811941964285714, 0.6021205357142857, 0.5811941964285714, 0.5940290178571429, 0.5931919642857143]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5264246323529411, 0.5356158088235294, 0.5629595588235294, 0.5650275735294118, 0.5685891544117647, 0.5853630514705882, 0.5903033088235294, 0.5767463235294118, 0.5855928308823529, 0.5845588235294118, 0.5860523897058824, 0.5901884191176471, 0.5899586397058824, 0.5835248161764706, 0.5942095588235294, 0.5854779411764706, 0.5782398897058824, 0.5916819852941176, 0.5893841911764706, 0.5929457720588235, 0.5846737132352942, 0.5813419117647058, 0.578125, 0.5826056985294118, 0.5936351102941176, 0.5894990808823529, 0.6014476102941176, 0.5822610294117647, 0.5854779411764706, 0.5885799632352942, 0.5849034926470589, 0.5984604779411765, 0.5846737132352942, 0.5950137867647058, 0.5814568014705882, 0.5766314338235294, 0.5752527573529411, 0.5827205882352942, 0.5804227941176471, 0.5843290441176471, 0.5818014705882353, 0.5815716911764706, 0.5819163602941176, 0.591796875, 0.5920266544117647, 0.5851332720588235, 0.5882352941176471, 0.6027113970588235, 0.5993795955882353, 0.5838694852941176, 0.5863970588235294, 0.5998391544117647, 0.5921415441176471, 0.5880055147058824, 0.5901884191176471, 0.5889246323529411, 0.5965073529411765, 0.5923713235294118, 0.5913373161764706, 0.5773207720588235, 0.583984375, 0.5924862132352942, 0.5940946691176471, 0.5959329044117647, 0.5903033088235294, 0.5915670955882353, 0.5893841911764706, 0.5942095588235294, 0.5854779411764706, 0.5885799632352942, 0.5886948529411765, 0.5809972426470589, 0.5885799632352942, 0.5882352941176471, 0.5971966911764706, 0.5916819852941176, 0.5905330882352942, 0.5911075367647058, 0.5894990808823529, 0.6011029411764706, 0.5955882352941176, 0.5896139705882353, 0.5943244485294118, 0.5843290441176471, 0.5908777573529411, 0.5838694852941176, 0.5916819852941176, 0.5921415441176471, 0.5874310661764706, 0.587890625, 0.5886948529411765, 0.5888097426470589, 0.5847886029411765, 0.5926011029411765, 0.5944393382352942, 0.5906479779411765, 0.5853630514705882, 0.5840992647058824, 0.5870863970588235, 0.5909926470588235]\n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "x0 (InputLayer)              (None, 28, 2782)          0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "lstm_5 (LSTM)                (None, 28, 128)           1490432                                                         \n",
      "_________________________________________________________________                                                      \n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_13 (Batc (None, 28, 128)           512                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_17 (Dropout)         (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "attention_5 (Attention)      (None, 128)               156                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_17 (Dense)             (None, 128)               16512                                                           \n",
      "_________________________________________________________________                                                      \n",
      "dropout_18 (Dropout)         (None, 128)               0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_18 (Dense)             (None, 64)                8256                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_19 (Dropout)         (None, 64)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_14 (Batc (None, 64)                256                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_19 (Dense)             (None, 32)                2080                                                            \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_15 (Batc (None, 32)                128                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_20 (Dropout)         (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_20 (Dense)             (None, 2)                 66                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 1,518,398                                                                                                \n",
      "Trainable params: 1,517,950                                                                                            \n",
      "Non-trainable params: 448                                                                                              \n",
      "_________________________________________________________________                                                      \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5164292279411765, 0.544921875, 0.5628446691176471, 0.5619255514705882, 0.5641084558823529, 0.568359375, 0.5691636029411765, 0.5681295955882353, 0.5676700367647058, 0.5688189338235294, 0.5687040441176471, 0.5657169117647058, 0.5642233455882353, 0.5685891544117647, 0.5653722426470589, 0.5674402573529411, 0.5688189338235294, 0.5674402573529411, 0.5681295955882353, 0.568359375, 0.5688189338235294, 0.568359375, 0.568359375, 0.568359375, 0.568359375, 0.5682444852941176, 0.5667509191176471, 0.5687040441176471, 0.5677849264705882, 0.5682444852941176, 0.5689338235294118, 0.5690487132352942, 0.5678998161764706, 0.5693933823529411, 0.5690487132352942, 0.5689338235294118, 0.5689338235294118, 0.5687040441176471, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118, 0.5689338235294118]\n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "x0 (InputLayer)              (None, 28, 2782)          0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "lstm_6 (LSTM)                (None, 28, 128)           1490432                                                         \n",
      "_________________________________________________________________                                                      \n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_16 (Batc (None, 28, 128)           512                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_21 (Dropout)         (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "attention_6 (Attention)      (None, 128)               156                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_21 (Dense)             (None, 64)                8256                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_22 (Dropout)         (None, 64)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_22 (Dense)             (None, 64)                4160                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_23 (Dropout)         (None, 64)                0                                                               \n",
      "_________________________________________________________________                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_17 (Batc (None, 64)                256                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_23 (Dense)             (None, 16)                1040                                                            \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_18 (Batc (None, 16)                64                                                              \n",
      "_________________________________________________________________                                                      \n",
      "dropout_24 (Dropout)         (None, 16)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_24 (Dense)             (None, 2)                 34                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 1,504,910                                                                                                \n",
      "Trainable params: 1,504,494                                                                                            \n",
      "Non-trainable params: 416                                                                                              \n",
      "_________________________________________________________________                                                      \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.43191964285714285, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.43191964285714285, 0.43191964285714285, 0.43191964285714285, 0.5680803571428571, 0.43191964285714285, 0.5680803571428571]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5067784926470589, 0.5166590073529411, 0.5155101102941176, 0.5219439338235294, 0.5224034926470589, 0.5459558823529411, 0.5434283088235294, 0.5342371323529411, 0.5358455882352942, 0.5140165441176471, 0.5363051470588235, 0.53515625, 0.5045955882352942, 0.5109145220588235, 0.5064338235294118, 0.5064338235294118, 0.5139016544117647, 0.5093060661764706, 0.5056295955882353, 0.5096507352941176, 0.5281479779411765, 0.5081571691176471, 0.5236672794117647, 0.5099954044117647, 0.5139016544117647, 0.513671875, 0.5072380514705882, 0.5310202205882353, 0.5193014705882353, 0.5227481617647058, 0.5299862132352942, 0.5503216911764706, 0.5481387867647058, 0.5519301470588235, 0.5541130514705882, 0.5534237132352942, 0.5374540441176471, 0.5358455882352942, 0.5094209558823529, 0.5181525735294118, 0.5101102941176471, 0.5044806985294118, 0.5090762867647058, 0.53125, 0.5426240808823529, 0.5647977941176471, 0.560546875, 0.5603170955882353, 0.5637637867647058, 0.5558363970588235, 0.5668658088235294, 0.5685891544117647, 0.5687040441176471, 0.5667509191176471, 0.5690487132352942, 0.5661764705882353, 0.5684742647058824, 0.5583639705882353, 0.5688189338235294, 0.5689338235294118, 0.5649126838235294, 0.5687040441176471, 0.5620404411764706, 0.568359375, 0.5559512867647058, 0.5680147058823529, 0.5688189338235294, 0.5665211397058824, 0.5674402573529411, 0.5690487132352942, 0.5676700367647058, 0.5672104779411765, 0.5684742647058824, 0.5658318014705882, 0.5637637867647058, 0.5674402573529411, 0.5689338235294118, 0.5474494485294118, 0.5626148897058824, 0.5629595588235294, 0.5653722426470589, 0.5583639705882353, 0.5515854779411765, 0.5523897058823529, 0.5642233455882353, 0.5670955882352942, 0.5689338235294118, 0.5678998161764706, 0.5676700367647058, 0.5613511029411765, 0.5525045955882353, 0.51953125, 0.5094209558823529, 0.5094209558823529, 0.4854090073529412, 0.4772518382352941, 0.4827665441176471, 0.5051700367647058, 0.5114889705882353, 0.47403492647058826]\n",
      "_________________________________________________________________                                                      \n",
      "Layer (type)                 Output Shape              Param #                                                         \n",
      "=================================================================                                                      \n",
      "x0 (InputLayer)              (None, 28, 2782)          0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "lstm_7 (LSTM)                (None, 28, 128)           1490432                                                         \n",
      "_________________________________________________________________                                                      \n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_19 (Batc (None, 28, 128)           512                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dropout_25 (Dropout)         (None, 28, 128)           0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "attention_7 (Attention)      (None, 128)               156                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_25 (Dense)             (None, 32)                4128                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_26 (Dropout)         (None, 32)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_26 (Dense)             (None, 64)                2112                                                            \n",
      "_________________________________________________________________                                                      \n",
      "dropout_27 (Dropout)         (None, 64)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_20 (Batc (None, 64)                256                                                             \n",
      "_________________________________________________________________                                                      \n",
      "dense_27 (Dense)             (None, 16)                1040                                                            \n",
      "_________________________________________________________________                                                      \n",
      "batch_normalization_21 (Batc (None, 16)                64                                                              \n",
      "_________________________________________________________________                                                      \n",
      "dropout_28 (Dropout)         (None, 16)                0                                                               \n",
      "_________________________________________________________________                                                      \n",
      "dense_28 (Dense)             (None, 2)                 34                                                              \n",
      "=================================================================                                                      \n",
      "Total params: 1,498,734                                                                                                \n",
      "Trainable params: 1,498,318                                                                                            \n",
      "Non-trainable params: 416                                                                                              \n",
      "_________________________________________________________________                                                      \n",
      " 30%|                                 | 6/20 [33:57<1:19:42, 341.58s/it, best loss: -0.6579241071428571]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x_train, x_test, y_train, y_test, batch_size, _  = data()\n",
    "    print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "    \n",
    "    best_run, best_model = optim.minimize(model=create_self_attention_lstm_model, data=data ,algo=tpe.suggest, \n",
    "                                          max_evals=20,trials=Trials(), notebook_name='4.0_old_Self_Attention_LSTM',rseed=1, verbose=False)\n",
    "    print(\"Evaluation of best performing model:\")\n",
    "    best_model.save(\"SELF_ATTENTION_LSTM_bestmodel.h5\")\n",
    "    print(best_model.get_config())\n",
    "    test_score, test_accuracy = best_model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('test_score: ', test_score, ' test_accuracy: ', test_accuracy)\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "GSPC_7days_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
